import streamlit as st
from tensorflow import keras
from streamlit_option_menu import option_menu
import pandas as pd
import requests
import pickle
import numpy as np
from tensorflow.keras.models import load_model
import tensorflow as tf
import os
import gdown

file_id2 = "1DieIObGn0_1-bpsy5Lum497vl3EY9K5O"
output2 = "Image_classify.keras"  # ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å

# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å Google Drive
gdown.download(f"https://drive.google.com/uc?id={file_id2}", output2, quiet=False, verify=False)

# ‡πÉ‡∏™‡πà FILE_ID ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Google Drive
file_id = "1ggTHWuxw2-I5jcInLhGDH7Zy8VIGa5IQ"
output = "movie_data.pkl"  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ã‡∏ü

# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå
gdown.download(f"https://drive.google.com/uc?id={file_id}", output, quiet=False)

print(f"Downloaded {output} successfully!")
with open("movie_data.pkl", "rb") as f:
    model = pickle.load(f)

EXAMPLE_NO = 1

def streamlit_menu():
    # ‡πÉ‡∏ä‡πâ sidebar menu ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        with st.sidebar:
            selected = option_menu(
                menu_title="Main Menu",  # required
                options=["Machine Learning", "Neural Network", "Demo Machine Learning", "Demo Neural Network"],  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏ô‡πâ‡∏≤ About
                # icons=["house", "book", "envelope", "info-circle"],  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏≠‡∏Ñ‡∏≠‡∏ô
                menu_icon="cast",  # optional
                default_index=0,  # optional
            )
        return selected

selected = streamlit_menu()

if selected == "Machine Learning":
    st.title("Welcome to the Machine Learning Page")
    
    
    
    
    st.markdown(
    "<h1 style='font-size:24px;'>üîπ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô</h1>",
    unsafe_allow_html=True
    )
    st.markdown("""
    1Ô∏è‚É£ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏û‡∏¢‡∏ô‡∏ï‡∏£‡πå<br>
    2Ô∏è‚É£ ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô<br>
    3Ô∏è‚É£ ‡πÉ‡∏ä‡πâ AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô<br>
    4Ô∏è‚É£ ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏´‡∏ô‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô 10 ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á
    """,unsafe_allow_html=True)
   
    code = '''import pandas as pd
import numpy as np
import ast
from sklearn.feature_extraction.text import TfidfVectorizer
        '''
    st.code(code, language="python")
    
    st.write("1.‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ<br>"
    "pandas : ‡πÉ‡∏ä‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏£‡∏≤‡∏á<br>"
    "numpy : ‡πÉ‡∏ä‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç<br>"
    "ast : ‡πÉ‡∏ä‡πâ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• string ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô list ‡∏´‡∏£‡∏∑‡∏≠ dictionary<br>"
    "TfidfVectorizer : ‡πÉ‡∏ä‡πâ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏ö TF-IDF ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á‡∏Å‡∏±‡∏ô"
    ,unsafe_allow_html=True)
    
    code = '''credits = pd.read_csv('tmdb_5000_credits.csv')
movies = pd.read_csv('tmdb_5000_movies.csv')
        '''
    st.code(code, language="python")
    st.write("2. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏û‡∏¢‡∏ô‡∏ï‡∏£‡πå<br>"
    "credits.csv ‚Üí ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö ‡∏ô‡∏±‡∏Å‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏•‡∏∞‡∏ó‡∏µ‡∏°‡∏á‡∏≤‡∏ô ‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡∏±‡∏á<br>"
    "movies.csv ‚Üí ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á, ‡πÅ‡∏ô‡∏ß‡∏´‡∏ô‡∏±‡∏á, ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏¢‡πà‡∏≠ ‡πÅ‡∏•‡∏∞‡∏≠‡∏∑‡πà‡∏ô ‡πÜ"
    )
    
    code = '''movies = movies.merge(credits, left_on='title', right_on='title')
        '''
    st.code(code, language="python")
    st.write("‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏™‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏†‡∏≤‡∏û‡∏¢‡∏ô‡∏ï‡∏£‡πå (title) ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏µ‡∏¢‡πå")
    
    code = '''movies = movies[['movie_id', 'title', 'overview', 'genres', 'keywords', 'cast', 'crew']]
        '''
    st.code(code, language="python")
    st.write("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥")
    
    code = '''def convert(obj):
    L = []
    for i in ast.literal_eval(obj):
        L.append(i['name'])
    return L
        '''
    st.code(code, language="python")
    st.markdown("‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON (‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö String) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô List ‡∏Ç‡∏≠‡∏á‡∏ä‡∏∑‡πà‡∏≠<br>"
    "ast.literal_eval() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á String ‚Üí List ‡∏Ç‡∏≠‡∏á Dictionary"
    ,unsafe_allow_html=True)

    
    code = '''movies['genres'] = movies['genres'].apply(convert)
movies['keywords'] = movies['keywords'].apply(convert)
        '''
    st.code(code, language="python")
    st.write("convert() ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß ‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå genres ‡πÅ‡∏•‡∏∞ keywords ‡∏à‡∏≤‡∏Å String ‚Üí List ‡∏Ç‡∏≠‡∏á‡∏ä‡∏∑‡πà‡∏≠")
    
    code = '''movies['cast'] = movies['cast'].apply(lambda x: [i['name'] for i in ast.literal_eval(x)[:3]])
        '''
    st.code(code, language="python")
    st.write("‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 3 ‡∏ô‡∏±‡∏Å‡πÅ‡∏™‡∏î‡∏á‡∏´‡∏•‡∏±‡∏Å ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON")
    
    code = '''movies['crew'] = movies['crew'].apply(lambda x: [i['name'] for i in ast.literal_eval(x) if i['job'] == 'Director'])
movies['tags'] = movies['genres'] + movies['keywords'] + movies['cast'] + movies['crew']
        '''
    st.code(code, language="python")
    st.write("‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏Å‡∏≥‡∏Å‡∏±‡∏ö ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡∏°‡∏á‡∏≤‡∏ô (crew)")
    
    code = '''movies['tags'] = movies['tags'].apply(lambda x: " ".join(x))
        '''
    st.code(code, language="python")
    st.write("‡∏£‡∏ß‡∏° ‡πÅ‡∏ô‡∏ß‡∏†‡∏≤‡∏û‡∏¢‡∏ô‡∏ï‡∏£‡πå (genres), ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (keywords), ‡∏ô‡∏±‡∏Å‡πÅ‡∏™‡∏î‡∏á (cast) ‡πÅ‡∏•‡∏∞‡∏ú‡∏π‡πâ‡∏Å‡∏≥‡∏Å‡∏±‡∏ö (crew) ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô tags ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥")
    
    code = '''from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(movies['tags'])
        '''
    st.code(code, language="python")
    st.write("‡πÉ‡∏ä‡πâ TF-IDF Vectorizer ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥ ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏ô‡∏±‡∏á‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏≠‡∏≠‡∏Å ‡πÄ‡∏ä‡πà‡∏ô the, and, is  ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå")
    
    code = '''from sklearn.metrics.pairwise import cosine_similarity
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

        '''
    st.code(code, language="python")
    st.write("‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Cosine Similarity ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á‡∏Å‡∏±‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏¢‡∏ô‡∏ï‡∏£‡πå"
    "‡∏Ñ‡πà‡∏≤‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 0 - 1<br>"
    "1.0 = ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏õ‡πä‡∏∞<br>"
    "0.0 = ‡πÑ‡∏°‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡πÄ‡∏•‡∏¢"
    ,unsafe_allow_html=True)
    
    code = '''def get_recommendations(title, cosine_sim=cosine_sim):
    idx = movies[movies['title'] == title].index[0]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:11]  # Get top 10 similar movies
    movie_indices = [i[0] for i in sim_scores]
    return movies['title'].iloc[movie_indices
        '''
    st.code(code, language="python")
    st.write("‚Ä¢	 ‡∏´‡∏≤‡∏ß‡πà‡∏≤‡∏´‡∏ô‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏¢‡∏π‡πà ‡∏î‡∏±‡∏ä‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£<br>"
    "‚Ä¢	‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡∏±‡∏á‡∏ó‡∏∏‡∏Å‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á<br>"
    "‚Ä¢	‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢<br>"
    "‚Ä¢	‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• 10 ‡∏´‡∏ô‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î"
    ,unsafe_allow_html=True)
    
    
    
    
    
if selected == "Neural Network":
    st.title("Image fruit veg classify")

    # ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
    st.markdown(
    "<h1 style='font-size:24px;'>üîπ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô</h1>",
    unsafe_allow_html=True
    )
    st.write("1Ô∏è‚É£ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏±‡∏Å‡∏ú‡∏•‡πÑ‡∏°‡πâ<br>"
    "2Ô∏è‚É£ ‡πÉ‡∏ä‡πâ AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏¢‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó ‡∏ú‡∏±‡∏Å ‡∏ú‡∏•‡πÑ‡∏°‡πâ‡πÅ‡∏•‡∏∞‡∏ö‡∏≠‡∏Å‡∏ä‡∏∑‡πà‡∏≠<br>"
    "3Ô∏è‚É£ ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤ ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ (accuracy)<br>"
    ,unsafe_allow_html=True)
     
    code = '''import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.applications import EfficientNetB0
import matplotlib.pyplot as plt
        '''
    st.code(code, language="python")
    st.write("‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• Deep Learning ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ TensorFlow ‡πÅ‡∏•‡∏∞ Keras ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ Matplotlib")
    
    code = '''data_train_path = 'Fruits_Vegetables/train
data_val_path = 'Fruits_Vegetables/validation
data_test_path = 'Fruits_Vegetables/test
        '''
    st.code(code, language="python")
    
    st.write("‡∏Å‡∏≥‡∏´‡∏ô‡∏î path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô training, ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á, ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Deep Learning")
    code = '''data_augmentation = keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
])

dataset_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=img_size,
    batch_size=batch_size
)

dataset_val = tf.keras.utils.image_dataset_from_directory(
    data_val_path,
    shuffle=False,
    image_size=img_size,
    batch_size=batch_size
)
        '''
    st.code(code, language="python")
    
    st.write("‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•  ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á Deep Learning ")
    code = '''base_model = EfficientNetB0(include_top=False, input_shape=(180, 180, 3), weights='imagenet')
base_model.trainable = False  # ‡∏•‡πá‡∏≠‡∏Ñ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
inputs = keras.Input(shape=(180, 180, 3))
x = data_augmentation(inputs)
x = base_model(x, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.2)(x)
outputs = layers.Dense(len(dataset_train.class_names), activation="softmax")(x)

model = keras.Model(inputs, outputs)
        '''
    st.code(code, language="python")
   
    st.write("‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Deep Learning‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ EfficientNetB0 ‡πÄ‡∏õ‡πá‡∏ô base model ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏•‡πÄ‡∏¢‡∏≠‡∏£‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó")
    code = '''model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
epochs = 50
history = model.fit(dataset_train, validation_data=dataset_val, epochs=epochs)
        '''
    st.code(code, language="python")
    
    st.markdown(
    "**‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏°‡πÑ‡∏û‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å Model**<br>"
    "optimizer=keras.optimizers.Adam(learning_rate=0.001):<br> ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ Adam ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ö‡∏Å‡∏≥‡∏´‡∏ô‡∏î learning rate ‡πÄ‡∏õ‡πá‡∏ô 0.001<br>"
    "<br>"
    "loss='sparse_categorical_crossentropy':<br> ‡πÉ‡∏ä‡πâ loss function ‡πÅ‡∏ö‡∏ö sparse categorical crossentropy<br>"
    "<br>"
    "metrics=['accuracy']: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î accuracy ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å<br>"
    "epochs = 50: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å ‡πÄ‡∏õ‡πá‡∏ô 50 ‡∏£‡∏≠‡∏ö<br>"
    "history = model.fit(dataset_train, validation_data=dataset_val, epochs=epochs):<br> ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å dataset_train ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á dataset_val",
    unsafe_allow_html=True)

    
    code = '''
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.show()
        '''
    st.code(code, language="python")
    st.write("‡πÉ‡∏ä‡πâ Matplotlib ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
    
    code = '''# ‡∏õ‡∏•‡∏î‡∏•‡πá‡∏≠‡∏Ñ‡∏ö‡∏≤‡∏á‡∏ä‡∏±‡πâ‡∏ô‡∏Ç‡∏≠‡∏á EfficientNetB0 ‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å‡∏ï‡πà‡∏≠ (Fine-tuning)
base_model.trainable = True
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

fine_tune_epochs = 5
history_fine = model.fit(dataset_train, validation_data=dataset_val, epochs=fine_tune_epochs)
        '''
    st.code(code, language="python")
    st.write("‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏õ‡∏•‡∏î‡∏•‡πá‡∏≠‡∏Ñ‡πÄ‡∏•‡πÄ‡∏¢‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á base model ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ù‡∏∂‡∏Å‡πÑ‡∏î‡πâ ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤")
    
    code = '''model.save("fruit_veg_classifier.h5")
        '''
    st.code(code, language="python")
    st.write("‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ù‡∏∂‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏∑‡πà‡∏≠ fruit_veg_classifier.h5 ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô save ‡∏Ç‡∏≠‡∏á Keras")
    
    
    
    
if selected == "Demo Machine Learning":
    st.title("Movie Recommendation System")
    # Load the processed data and similarity matrix
    with open('movie_data.pkl', 'rb') as file:
        movies, cosine_sim = pickle.load(file)

    # Function to get movie recommendations
    def get_recommendations(title, cosine_sim=cosine_sim):
        idx = movies[movies['title'] == title].index[0]
        sim_scores = list(enumerate(cosine_sim[idx]))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
        sim_scores = sim_scores[1:11]  # Get top 10 similar movies
        movie_indices = [i[0] for i in sim_scores]
        return movies[['title', 'movie_id']].iloc[movie_indices]

    # Fetch movie poster from TMDB API
    def fetch_poster(movie_id):
        api_key = '7b995d3c6fd91a2284b4ad8cb390c7b8'  # Replace with your TMDB API key
        url = f'https://api.themoviedb.org/3/movie/{movie_id}?api_key={api_key}'
        
        response = requests.get(url)
        data = response.json()
        print(data)
        poster_path = data.get('poster_path', 'default_value')
        
        
        if poster_path:
            full_path = f"https://image.tmdb.org/t/p/w500{poster_path}"
            return full_path
        else:
            return 'default_poster_path'  # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏û‡∏ö poster_path

    selected_movie = st.selectbox("Select a movie:", movies['title'].values)

    if st.button('Recommend'):
        recommendations = get_recommendations(selected_movie)
        st.write("Top 10 recommended movies:")

        # Create a 2x5 grid layout
        for i in range(0, 10, 5):  # Loop over rows (2 rows, 5 movies each)
            cols = st.columns(5)  # Create 5 columns for each row
            for col, j in zip(cols, range(i, i+5)):
                if j < len(recommendations):
                    movie_title = recommendations.iloc[j]['title']
                    movie_id = recommendations.iloc[j]['movie_id']
                    poster_url = fetch_poster(movie_id)
                    with col:
                        st.image(poster_url, width=130)
                        st.write(movie_title)
if selected == "Demo Neural Network":  
    
    st.title("Demo Neural Network")
    st.header('Image Classification Model')
    



    # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Keras
    model = tf.keras.models.load_model(output2)
    data_cat = ['apple',
    'banana',
    'beetroot',
    'bell pepper',
    'cabbage',
    'capsicum',
    'carrot',
    'cauliflower',
    'chilli pepper',
    'corn',
    'cucumber',
    'eggplant',
    'garlic',
    'ginger',
    'grapes',
    'jalepeno',
    'kiwi',
    'lemon',
    'lettuce',
    'mango',
    'onion',
    'orange',
    'paprika',
    'pear',
    'peas',
    'pineapple',
    'pomegranate',
    'potato',
    'raddish',
    'soy beans',
    'spinach',
    'sweetcorn',
    'sweetpotato',
    'tomato',
    'turnip',
    'watermelon']
    img_height = 180
    img_width = 180

    # ‡πÉ‡∏ä‡πâ st.file_uploader ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå
    uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])

    if uploaded_file is not None:
        # ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î
        image_load = tf.keras.utils.load_img(uploaded_file, target_size=(img_height, img_width))
        img_arr = tf.keras.utils.img_to_array(image_load)
        img_bat = tf.expand_dims(img_arr, 0)

        # ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
        predict = model.predict(img_bat)
        score = tf.nn.softmax(predict)

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        st.image(image_load, width=200)
        st.write('Veg/Fruit in image is ' + data_cat[np.argmax(score)])
        st.write('With accuracy of ' + str(np.max(score) * 100) + '%')





















